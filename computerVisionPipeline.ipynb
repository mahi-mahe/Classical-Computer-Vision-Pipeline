{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15002ada",
   "metadata": {},
   "source": [
    "Step1- Image Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aaeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Classical Computer Vision Pipeline for Face Recognition\n",
    "#Capture → Clean → Segment → Extract → Match → Recognize → Refine\n",
    "\n",
    "import cv2\n",
    "# Option 1: Acquire from webcam (real-time)\n",
    "cap = cv2.VideoCapture(0)  # 0 = default camera\n",
    "ret, frame = cap.read()    # Capture a frame\n",
    "if ret:\n",
    "    cv2.imwrite('pac (1).jpg', frame)  # Save for next steps\n",
    "cap.release()\n",
    "\n",
    "# Option 2: Load from file (simulated acquisition)\n",
    "# frame = cv2.imread('input.jpg')  # Use this if no camera\n",
    "cv2.imshow('Acquired Image', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classical Computer Vision Pipeline for Face Recognition\n",
    "#Capture → Clean → Segment → Extract → Match → Recognize → Refine\n",
    "\n",
    "import cv2\n",
    "# Option 1: Acquire from webcam (real-time)\n",
    "frame = cv2.imread('face.jpg')\n",
    "\n",
    "# Option 2: Load from file (simulated acquisition)\n",
    "# frame = cv2.imread('input.jpg')  # Use this if no camera\n",
    "cv2.imshow('Acquired Image', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cab7f6",
   "metadata": {},
   "source": [
    "Step-2 Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f40796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise RuntimeError(\"Failed to capture frame from camera\")\n",
    "\n",
    "cv2.imwrite(\"acquired_image.jpg\", frame)  # Make sure this succeeds\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('acquired_image.jpg')\n",
    "#Enhance image for better feature extraction\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Grayscale\n",
    "blur = cv2.GaussianBlur(gray, (5,5), 0)       # Denoise\n",
    "equalized = cv2.equalizeHist(blur)            # Contrast enhancement\n",
    "cv2.imshow('Preprocessed', equalized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('preprocessed.jpg', equalized)    # Save for next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca0090",
   "metadata": {},
   "source": [
    "Step3 - REGION OF INTEREST(ROI) / SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('preprocessed.jpg', 0)\n",
    "#Separate foreground (face) from background\n",
    "_, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)  # Binary segmentation\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "output = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(output, contours, -1, (0,255,0), 2)  # Draw segmented regions\n",
    "\n",
    "cv2.imshow('Segmented', output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('segmented.jpg', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3bf33",
   "metadata": {},
   "source": [
    "Step4 - FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b037972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('segmented.jpg', 0)\n",
    "orb = cv2.ORB_create()\n",
    "#Describe important points\n",
    "keypoints, descriptors = orb.detectAndCompute(img, None)  # Extract features\n",
    "output = cv2.drawKeypoints(img, keypoints, None, color=(0,255,0))\n",
    "cv2.imshow('Features', output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# Descriptors are arrays; save keypoints if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d44b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read image (grayscale)\n",
    "img = cv2.imread('segmented.jpg', 0)\n",
    "\n",
    "# ORB feature detector\n",
    "orb = cv2.ORB_create()\n",
    "keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "\n",
    "# Draw keypoints (green)\n",
    "output = cv2.drawKeypoints(\n",
    "    img,\n",
    "    keypoints,\n",
    "    None,\n",
    "    color=(0, 255, 0),\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "\n",
    "# Convert BGR → RGB for matplotlib\n",
    "output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display using matplotlib\n",
    "plt.imshow(output_rgb)\n",
    "plt.title(f\"ORB Features (Keypoints = {len(keypoints)})\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b166d4",
   "metadata": {},
   "source": [
    "Step5 - FEATURE MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "img1 = cv2.imread('segmented.jpg', 0)\n",
    "img2 = cv2.imread('preprocessed.jpg', 0)\n",
    "\n",
    "if img1 is None:\n",
    "    raise IOError(\"segmented.jpg not found or cannot be read\")\n",
    "if img2 is None:\n",
    "    raise IOError(\"another_image.jpg not found or cannot be read\")\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "# Check descriptors\n",
    "if des1 is None or des2 is None:\n",
    "    raise ValueError(\n",
    "        f\"No descriptors found: des1={des1 is None}, des2={des2 is None}. \"\n",
    "        \"Try different images or ORB parameters.\"\n",
    "    )\n",
    "\n",
    "print(\"des1 shape:\", des1.shape, \"dtype:\", des1.dtype)\n",
    "print(\"des2 shape:\", des2.shape, \"dtype:\", des2.dtype)\n",
    "\n",
    "if des1.dtype != des2.dtype or des1.shape[1] != des2.shape[1]:\n",
    "    raise ValueError(\"Descriptor types or dimensions do not match\")\n",
    "\n",
    "#Connect same object between two images. It compares descriptor A with B and finds the closest match\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "output = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None)\n",
    "\n",
    "cv2.imshow('Matches', output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8d402",
   "metadata": {},
   "source": [
    "Step 6 -Classification / Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be979191",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"segmented.jpg\"\n",
    "if not os.path.exists(img_path):\n",
    "    raise FileNotFoundError(f\"{img_path} not found\")\n",
    "\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "if img is None:\n",
    "    raise IOError(f\"Failed to read {img_path}\")\n",
    "\n",
    "# Find contours on a binary mask (segmented.jpg)\n",
    "contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "output = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "min_area = 50      \n",
    "large_thresh = 1000  # if area > large_thresh → \"Large\", else \"Small\"\n",
    "\n",
    "for i, cnt in enumerate(contours):\n",
    "    area = cv2.contourArea(cnt)\n",
    "    if area < min_area:\n",
    "        continue   \n",
    "\n",
    "    # Compute bounding box for more stable label placement\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "    label_cls = \"Large\" if area > large_thresh else \"Small\"\n",
    "    label = f\"{label_cls}, A={int(area)}\"\n",
    "\n",
    "    # Draw contour\n",
    "    cv2.drawContours(output, [cnt], -1, (0, 255, 0), 2)\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(output, (x, y), (x + w, y + h), (0, 0, 255), 1)\n",
    "    # Put label above the box\n",
    "    cv2.putText(output, label, (x, max(y - 5, 0)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "print(f\"Kept {len(contours)} contours (before area filter).\")\n",
    "\n",
    "cv2.imshow(\"Classified\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4250a5",
   "metadata": {},
   "source": [
    "Step7 - Feature Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example detections: [x, y, w, h, confidence]\n",
    "detections = np.array([\n",
    "    [100, 100, 50, 50, 0.9],\n",
    "    [105, 105, 50, 50, 0.8]\n",
    "])\n",
    "\n",
    "# Apply Non-Maximum Suppression\n",
    "boxes = detections[:, :4].tolist()\n",
    "scores = detections[:, 4].tolist()\n",
    "\n",
    "indices = cv2.dnn.NMSBoxes(boxes, scores, score_threshold=0.5, nms_threshold=0.4) #Remove duplicate detections, if multiple boxes overlap then  keep strongest\n",
    "\n",
    "# Load image safely\n",
    "img = cv2.imread('segmented.jpg')\n",
    "if img is None:\n",
    "    raise FileNotFoundError(\"segmented.jpg not found.\")\n",
    "\n",
    "# Draw bounding boxes after NMS\n",
    "if len(indices) > 0:\n",
    "    for i in indices.flatten():  # Flatten ensures correct indexing\n",
    "        x, y, w, h = detections[i, :4].astype(int)\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "print(f\"Detected objects after NMS: {len(indices)}\")\n",
    "\n",
    "cv2.imshow('Post-processed Output', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Haar Cascades\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'    #Use trained model for face and eyes detection, etc. These XML files contain pre-trained classifiers for various objects, including faces and eyes.\n",
    ")\n",
    "\n",
    "eye_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_eye.xml'\n",
    ")\n",
    "\n",
    "# Use webcam (change to image path if needed)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Cannot access camera.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw face rectangle\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Detect eyes inside face ROI\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(\n",
    "                roi_color,\n",
    "                (ex, ey),\n",
    "                (ex + ew, ey + eh),\n",
    "                (0, 255, 0),\n",
    "                1\n",
    "            )\n",
    "\n",
    "    cv2.imshow('Classical Face + Eye Detection', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
